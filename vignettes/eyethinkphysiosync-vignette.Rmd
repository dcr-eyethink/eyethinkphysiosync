---
title: "eyethinkphysiosync-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{eyethinkphysiosync-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This is a toolbox for processing and analysing data from emotibit sensors. It will collate the physio data, re running heart rate analyses with heart py, if required. It will then associate the data with logs, obsersvational data, or output from a gorilla experiment, labeling the stimuli and attaching behavioural data. Finally, it contains tools for cross recurrence quantification analysis. It uses the eyethinkdata package, which will be installed if you don't have it.

Here's how to get the latest version (always recommended as there's a fair bit of development going on)

```{r eval=FALSE}
devtools::install_github("dcr-eyethink/eyethinkphysiosync")
```

```{r setup}
library(eyethinkphysiosync)
```

There are three main stages to this process: processing the raw emotibit data, trimming and integrating the data with a record of participants' experiences and behaviour, and then optionally running a cross recurrence analysis. In each case, there are single wrapper functions to achieve each of these in one command, but in this vignette I'll walk through the steps to show you the process. 

We will be processing data from a little experiment in which three participants watched 60 second advert and a 2 minute clip of a drama show.

## Processing raw emotibit data

To harvest the data from emotbits, look on the sensor SD card for all files created during the experiment period (the dates are encoded in filenames). Copy all the files (.csv and .json) from the SD cards into one data folder on computer. 

### Compiling and processing emotibit data

The first step in processing the data is to place the files into folders that are named after the sensors. This can be the serial number that pops up on the osciloscope, or we can translate that code into a sensor name. By default, the function uses the sensor names from the eyethink lab, but a differnt .csv filename can be supplied. It will also default to duplicating the datafolder (to emotibit_data in working directory), but if you want to overwrite/reorganize the original folder, set destination = NULL.

```{r}
emotibit_session_folders(datafolder = "raw_emotibit",sensor_id = "eyethink")
```

Now we need to use the emotibit data parser to generate different .csv files for each measurement. If the function can't find your emotibit data parser app, it will ask for the location. During this process, the app will pop up a window to inform you of its progress

```{r message=FALSE, warning=FALSE}
emotibit_parser(datafolder = "emotibit_data")
```

Next we can reprocess the heart rate signal using heart py and the PG sensor data. This may not be necessary if/when the emotibit uses a better algorithm on its chip. 

```{r}
emotibit_heartpy(datafolder = "emotibit_data")
```

Finally, we need to compile all the data for the measurements that we care about. This function goes through all the emotibit data folders and compiles the data for the DVs that are named. It saves the data out to a file, emotibit_data_comb.csv, and returns it as a datatable named ed

```{r}
processed_emotibit_data <- emotibit_compile(edv = c("HR","EA","ACC"),datafolder = "emotibit_data")
```

It also returns various import checks and plots of the data. For example, below we show a histogram of the start times for the three sessions

```{r}
processed_emotibit_data$p_start
```

### Wrapper function

You can achieve everything above in one function call, that just runs through the processes in turn:

```{r eval=F}
processed_emotibit <- emotibit_process(datafolder = "raw_emotibit")
```

## Behavioural data

Now we can gather the behavioural data so we know what the participants were doing and seeing, and importantly, when. We want this trial data in the form of a data.table, typically called td, that has one row per person, per stimulus that they saw. This could be one of many 30 sec ads a person sees, or a 2 hour movie that each person sees once. 

### Format of trial data and participant data

Here's an example of the minimal information that we need in the trial data

```{r}
fread("eg_td.csv")
```

The first 4 columns here are required for the processes we will carry out below:
**pid** the paritcipant ID
**eid** the name of the sensor that the participant wore
**starttime** The UTC time code (https://www.unixtimestamp.com) when the stimulus started
**stoptime** end time of the stimulus

In addition you need one or more columns to pick out a unique trial or stimulus event (something that each participant only sees once). This could be the stimulus name, for example. Later we will be carrying out a cross recurrence analysis on all participants who experienced this same event, and this is how we will identify them.

Typically, I use more than one column, so that I can include information to identify types of trial or conditions. By convention, I use category, condition, item and stim columns, as we see here. But whatever is there will be populated throughout the data we generate. You can also include here any behavioural information about response to the stimuli, such as rating ro memory tasks. This data can be entered by hand into a spreadsheet, or taken from a experiment log, or derived from gorilla data.

### Processing gorilla standardised task

Many of the experiments run in the eyethink lab use a standardized task called snappily *AV block multiple stim types*. There are functions to automatically process data from the various versions of this task to generate summary data.

```{r}
processed_gorilla <- gorilla_physio_presentation()
```

There are 2 outputs here. pd is the participant data, and looks like this for our toy experiment

```{r}
processed_gorilla$pd
```

And the trial data

```{r}
processed_gorilla$td
```

As well as the important trial start and stop times, we have the participants responses on the various rating tasks after each stimuli. We can now match up this trial information with the physiological data

### Linking physio and behavioural

The function that joins together the behavioural and emotibit data is called physio_merge(). It can take a while as there is a lot of processing. Before doing so, it's a good idea to run it with stop_at_plotcheck = T. This gives you a graphical representation of where you have emotibit and where you have gorilla data. Time is on the x axis, with days in different panels, and the different emotibit sensors are arrayed on the y axis. The emotibit sensor ids are read off from the SD cards for the emotibit data, and entered by the participant in the gorilla data. 

```{r}
physio_merge(processed_emotibit ,processed_gorilla,stop_at_plotcheck = T )
```

The black lines are where you have gorilla data (with the pids written nearby), and the red lines are the period for which you have physiological data. In a perfect world, all the red lines would appear over the black bars. But you may get times where the red lines don't fully cover the black areas, where the sensor battery gave out for example, or someone forgot to start recording. Another error that can occur is that the red lines will systematical be an hour off of the black bars in every case. This suggests that there has been some problem with the time zones. The emotibit chips don't handle British daylight savings very well. To correct for this, re run emotibit_compile() with hour_adjust = 1 or -1.

A further thing is look out for are times in which you have a black bar and a red line at the same time, but they aren't in top of each other. That has occurred at the top of this plot above. A reasonable explanation is that the participant has entered the eid incorrectly. Here they appear to have mistake the red case for an orange (to be fair, it is an orangey-red). Other typical mistakes are putting a 9 for a 6, for example. It's a good idea to confirm these errors by looking in the experiment log, where they experimenter should have noted the eid as well as a back up. 

There's a function to handle those corrections quickly. You just give it a data.table of pids and their correct eids, and it will go through your pd, td and any other data passed in a list to make correction. Then we can rerun the merging process and store it.

```{r}
processed_gorilla <- gorilla_eid_corrections(processed_gorilla, corrections=data.table(pid=c(10370290),eid=c("red4")))
data <- physio_merge(processed_emotibit ,processed_gorilla)
```

The physio_merge function spits out the plot to check your data alignment, as before, and also a plot of theeda data per person and emotibit sensor. There is a threshold (20 by default, shown as a line on the plot) such that if the mean EDA reading for someone is below that, we set filter=0 and they get excluded. Also we exclude people where the range of EDA values is small (default <0.03). In there cases that's either someone who is very sweaty or not at all sweaty, or a misplaced sensor. We also set filter=0 for individual measurements that are outside of thresholds (defaults <.01 & >25). In the trial data, td, we summarize how many measurements were excluded from each trial (you might want to throw out trials with less than a certain % of valid meaurements), and the average of those values for each person is reported as % in the plot. 

The main output of this function are the set of data.tables that give you the emotitbit measurements moment by moment (ed), the trial data (td) where those are summarized over trials, the participant data (pd) summarized over people and the stimuli data (sd) summarized over stimuli items.

You can look at the tiemcourse by plotting data$ed: 

```{r}
ggplot(data=data$ed,aes(x=tt,y=norm,colour=item))+geom_smooth()
```

However, using geom_smooth() sometimes results in too much smoothing. As an alternative for plotting, physio_merge() also outputs data$dm, which has each dv averaged over all participants in 1 second time bins, with the metrics of raw and morn scores. This can be plotted with geom_line()

```{r}
ggplot(data=data$dm[metric=="norm" & edv=="EA"],aes(x=t,y=y,colour=item))+geom_line()+theme_bw()
```

The whole data list of different data types can be saved out into separate .csv files with one command. It will put them in a folder called 'processed' by default, or you can specify a different name

```{r}
saveout_datalist(data)
```

If you are just interested in the behaviour and the physiological response of individuals, you can stop here. Or, in the next step, we take the ed data and calculate the physiological synchrony between these signals using cross recurrence analysis.

## Cross recurrence analysis

In this toolbox we use Dale and Coco's (21014) [CRQA methods paper and R toolbox](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2014.00510/full). This package adds a set of function that basically iterate the crqa() function over lots of trials and pairs of dyads. 
Here we have run the crqa_trials() function over our data of 3 participants watching two trials. We've turned on options to draw the pairs of time series and the cross recurrence plots for each pair of heart rate time series.

```{r}
   hr_crqa <-   crqa_trials(data=data,radius=.6,delay=2,embed=4,mindiagline=6,normalize=T,
                            edvs= list("HR"),drawplot=T,plot_ts=T)
```

The function returns crqa stats on every pairwise comparison for every trial.

```{r}
hr_crqa
```
Typically, we also want these pairwise comparisons data aggregated over trials, conditions or stimuli items. There is a wrapper function crqq_full() that will do this step too. Also, above we specified various crqa parameters like embedding and delay. From lots of experiences, we have a set of standard values that we use by default. The crqq_full() function runs a set of specified analyses using these defaults and returns the full rqa table, and updates our data list with averages over trials, people and stimuli items. By default it will run crqas for HR, EA and a multidemensional HR.EA combination, and windowed analyses. But you can also specify a subste of these. Here I've just asked for EA and windowed EA which give us time course information, since those are the measures that we have found are the best indicators of engagement. 

```{r}
data <- crqa_full(data=data,analyses = c("EA","EAwin"))
```

That probably took a matter of seconds, since we only have 3 people watching 2 stimuli. But since pairwise comparisons are at the heart of crqa, you'll find that processing time increases rapidly with increasing participant numbers and stimuli lengths. We now have all the pairwise comparisons stored in data\$r, we have windowd timecourse data in data\$rw, the trial information in data\$td has been updated with summary stats for each participant's trial, and we have averages for each stimuli item in data\$sd. We can now visualise these. 

### Visualising item and condition differences

The means for the trial can be plotted like any other DV such as accuracy or reaction time using data$td. Of course, with only 2 trial and three people these differences are meanginless here. 

```{r}
pirateye(data$td,dv="DET_EA",x_condition = "item", violin=F)
```

You could also look for correlations between their DET scores and their explicit ratings in the trial data, or between the item mean DET scores in data\$sd and some external measure of engagement or success (such as sales figures, youtube likes, etc). 



```{r}
ggplot(data=data$dm[metric=="DET" & edv=="EA"],aes(x=t,y=y,colour=item))+geom_line()+theme_bw()

```










